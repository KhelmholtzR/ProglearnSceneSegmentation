{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADE20K model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMa6NaSuuP8"
      },
      "source": [
        "## This model is sourced from https://github.com/CSAILVision/semantic-segmentation-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ntwUymrr58"
      },
      "source": [
        "import os\n",
        "import time\n",
        "# import math\n",
        "import random\n",
        "import argparse\n",
        "from distutils.version import LooseVersion\n",
        "# Numerical libs\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrGqmKeWuVfr"
      },
      "source": [
        "# train one epoch\n",
        "def train(segmentation_module, iterator, optimizers, history, epoch, cfg):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    ave_total_loss = AverageMeter()\n",
        "    ave_acc = AverageMeter()\n",
        "\n",
        "    segmentation_module.train(not cfg.TRAIN.fix_bn)\n",
        "\n",
        "    # main loop\n",
        "    tic = time.time()\n",
        "    for i in range(cfg.TRAIN.epoch_iters):\n",
        "        # load a batch of data\n",
        "        batch_data = next(iterator)\n",
        "        data_time.update(time.time() - tic)\n",
        "        segmentation_module.zero_grad()\n",
        "\n",
        "        # adjust learning rate\n",
        "        cur_iter = i + (epoch - 1) * cfg.TRAIN.epoch_iters\n",
        "        adjust_learning_rate(optimizers, cur_iter, cfg)\n",
        "\n",
        "        # forward pass\n",
        "        loss, acc = segmentation_module(batch_data)\n",
        "        loss = loss.mean()\n",
        "        acc = acc.mean()\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        for optimizer in optimizers:\n",
        "            optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - tic)\n",
        "        tic = time.time()\n",
        "\n",
        "        # update average loss and acc\n",
        "        ave_total_loss.update(loss.data.item())\n",
        "        ave_acc.update(acc.data.item()*100)\n",
        "\n",
        "        # calculate accuracy, and display\n",
        "        if i % cfg.TRAIN.disp_iter == 0:\n",
        "            print('Epoch: [{}][{}/{}], Time: {:.2f}, Data: {:.2f}, '\n",
        "                  'lr_encoder: {:.6f}, lr_decoder: {:.6f}, '\n",
        "                  'Accuracy: {:4.2f}, Loss: {:.6f}'\n",
        "                  .format(epoch, i, cfg.TRAIN.epoch_iters,\n",
        "                          batch_time.average(), data_time.average(),\n",
        "                          cfg.TRAIN.running_lr_encoder, cfg.TRAIN.running_lr_decoder,\n",
        "                          ave_acc.average(), ave_total_loss.average()))\n",
        "\n",
        "            fractional_epoch = epoch - 1 + 1. * i / cfg.TRAIN.epoch_iters\n",
        "            history['train']['epoch'].append(fractional_epoch)\n",
        "            history['train']['loss'].append(loss.data.item())\n",
        "            history['train']['acc'].append(acc.data.item())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELCy-3DSuc4f"
      },
      "source": [
        "def checkpoint(nets, history, cfg, epoch):\n",
        "    print('Saving checkpoints...')\n",
        "    (net_encoder, net_decoder, crit) = nets\n",
        "\n",
        "    dict_encoder = net_encoder.state_dict()\n",
        "    dict_decoder = net_decoder.state_dict()\n",
        "\n",
        "    torch.save(\n",
        "        history,\n",
        "        '{}/history_epoch_{}.pth'.format(cfg.DIR, epoch))\n",
        "    torch.save(\n",
        "        dict_encoder,\n",
        "        '{}/encoder_epoch_{}.pth'.format(cfg.DIR, epoch))\n",
        "    torch.save(\n",
        "        dict_decoder,\n",
        "        '{}/decoder_epoch_{}.pth'.format(cfg.DIR, epoch))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxFDrCIBugqO"
      },
      "source": [
        "def group_weight(module):\n",
        "    group_decay = []\n",
        "    group_no_decay = []\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            group_decay.append(m.weight)\n",
        "            if m.bias is not None:\n",
        "                group_no_decay.append(m.bias)\n",
        "        elif isinstance(m, nn.modules.conv._ConvNd):\n",
        "            group_decay.append(m.weight)\n",
        "            if m.bias is not None:\n",
        "                group_no_decay.append(m.bias)\n",
        "        elif isinstance(m, nn.modules.batchnorm._BatchNorm):\n",
        "            if m.weight is not None:\n",
        "                group_no_decay.append(m.weight)\n",
        "            if m.bias is not None:\n",
        "                group_no_decay.append(m.bias)\n",
        "\n",
        "    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n",
        "    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n",
        "    return groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMSDesj5ui5Y"
      },
      "source": [
        "def create_optimizers(nets, cfg):\n",
        "    (net_encoder, net_decoder, crit) = nets\n",
        "    optimizer_encoder = torch.optim.SGD(\n",
        "        group_weight(net_encoder),\n",
        "        lr=cfg.TRAIN.lr_encoder,\n",
        "        momentum=cfg.TRAIN.beta1,\n",
        "        weight_decay=cfg.TRAIN.weight_decay)\n",
        "    optimizer_decoder = torch.optim.SGD(\n",
        "        group_weight(net_decoder),\n",
        "        lr=cfg.TRAIN.lr_decoder,\n",
        "        momentum=cfg.TRAIN.beta1,\n",
        "        weight_decay=cfg.TRAIN.weight_decay)\n",
        "    return (optimizer_encoder, optimizer_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01qCMMxuk2G"
      },
      "source": [
        "def adjust_learning_rate(optimizers, cur_iter, cfg):\n",
        "    scale_running_lr = ((1. - float(cur_iter) / cfg.TRAIN.max_iters) ** cfg.TRAIN.lr_pow)\n",
        "    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder * scale_running_lr\n",
        "    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder * scale_running_lr\n",
        "\n",
        "    (optimizer_encoder, optimizer_decoder) = optimizers\n",
        "    for param_group in optimizer_encoder.param_groups:\n",
        "        param_group['lr'] = cfg.TRAIN.running_lr_encoder\n",
        "    for param_group in optimizer_decoder.param_groups:\n",
        "        param_group['lr'] = cfg.TRAIN.running_lr_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIV17goUunKG"
      },
      "source": [
        "def main(cfg, gpus):\n",
        "    # Network Builders\n",
        "    net_encoder = ModelBuilder.build_encoder(\n",
        "        arch=cfg.MODEL.arch_encoder.lower(),\n",
        "        fc_dim=cfg.MODEL.fc_dim,\n",
        "        weights=cfg.MODEL.weights_encoder)\n",
        "    net_decoder = ModelBuilder.build_decoder(\n",
        "        arch=cfg.MODEL.arch_decoder.lower(),\n",
        "        fc_dim=cfg.MODEL.fc_dim,\n",
        "        num_class=cfg.DATASET.num_class,\n",
        "        weights=cfg.MODEL.weights_decoder)\n",
        "\n",
        "    crit = nn.NLLLoss(ignore_index=-1)\n",
        "\n",
        "    if cfg.MODEL.arch_decoder.endswith('deepsup'):\n",
        "        segmentation_module = SegmentationModule(\n",
        "            net_encoder, net_decoder, crit, cfg.TRAIN.deep_sup_scale)\n",
        "    else:\n",
        "        segmentation_module = SegmentationModule(\n",
        "            net_encoder, net_decoder, crit)\n",
        "\n",
        "    # Dataset and Loader\n",
        "    dataset_train = TrainDataset(\n",
        "        cfg.DATASET.root_dataset,\n",
        "        cfg.DATASET.list_train,\n",
        "        cfg.DATASET,\n",
        "        batch_per_gpu=cfg.TRAIN.batch_size_per_gpu)\n",
        "\n",
        "    loader_train = torch.utils.data.DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=len(gpus),  # we have modified data_parallel\n",
        "        shuffle=False,  # we do not use this param\n",
        "        collate_fn=user_scattered_collate,\n",
        "        num_workers=cfg.TRAIN.workers,\n",
        "        drop_last=True,\n",
        "        pin_memory=True)\n",
        "    print('1 Epoch = {} iters'.format(cfg.TRAIN.epoch_iters))\n",
        "\n",
        "    # create loader iterator\n",
        "    iterator_train = iter(loader_train)\n",
        "\n",
        "    # load nets into gpu\n",
        "    if len(gpus) > 1:\n",
        "        segmentation_module = UserScatteredDataParallel(\n",
        "            segmentation_module,\n",
        "            device_ids=gpus)\n",
        "        # For sync bn\n",
        "        patch_replication_callback(segmentation_module)\n",
        "    segmentation_module.cuda()\n",
        "\n",
        "    # Set up optimizers\n",
        "    nets = (net_encoder, net_decoder, crit)\n",
        "    optimizers = create_optimizers(nets, cfg)\n",
        "\n",
        "    # Main loop\n",
        "    history = {'train': {'epoch': [], 'loss': [], 'acc': []}}\n",
        "\n",
        "    for epoch in range(cfg.TRAIN.start_epoch, cfg.TRAIN.num_epoch):\n",
        "        train(segmentation_module, iterator_train, optimizers, history, epoch+1, cfg)\n",
        "\n",
        "        # checkpointing\n",
        "        checkpoint(nets, history, cfg, epoch+1)\n",
        "\n",
        "    print('Training Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0csMZphusoh"
      },
      "source": [
        "def run():\n",
        "    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\n",
        "        'PyTorch>=0.4.0 is required'\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"PyTorch Semantic Segmentation Training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cfg\",\n",
        "        default=\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\",\n",
        "        metavar=\"FILE\",\n",
        "        help=\"path to config file\",\n",
        "        type=str,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gpus\",\n",
        "        default=\"0-3\",\n",
        "        help=\"gpus to use, e.g. 0-3 or 0,1,2,3\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"opts\",\n",
        "        help=\"Modify config options using the command-line\",\n",
        "        default=None,\n",
        "        nargs=argparse.REMAINDER,\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    cfg.merge_from_file(args.cfg)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    # cfg.freeze()\n",
        "\n",
        "    logger = setup_logger(distributed_rank=0)   # TODO\n",
        "    logger.info(\"Loaded configuration file {}\".format(args.cfg))\n",
        "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
        "\n",
        "    # Output directory\n",
        "    if not os.path.isdir(cfg.DIR):\n",
        "        os.makedirs(cfg.DIR)\n",
        "    logger.info(\"Outputing checkpoints to: {}\".format(cfg.DIR))\n",
        "    with open(os.path.join(cfg.DIR, 'config.yaml'), 'w') as f:\n",
        "        f.write(\"{}\".format(cfg))\n",
        "\n",
        "    # Start from checkpoint\n",
        "    if cfg.TRAIN.start_epoch > 0:\n",
        "        cfg.MODEL.weights_encoder = os.path.join(\n",
        "            cfg.DIR, 'encoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n",
        "        cfg.MODEL.weights_decoder = os.path.join(\n",
        "            cfg.DIR, 'decoder_epoch_{}.pth'.format(cfg.TRAIN.start_epoch))\n",
        "        assert os.path.exists(cfg.MODEL.weights_encoder) and \\\n",
        "            os.path.exists(cfg.MODEL.weights_decoder), \"checkpoint does not exitst!\"\n",
        "\n",
        "    # Parse gpu ids\n",
        "    gpus = parse_devices(args.gpus)\n",
        "    gpus = [x.replace('gpu', '') for x in gpus]\n",
        "    gpus = [int(x) for x in gpus]\n",
        "    num_gpus = len(gpus)\n",
        "    cfg.TRAIN.batch_size = num_gpus * cfg.TRAIN.batch_size_per_gpu\n",
        "\n",
        "    cfg.TRAIN.max_iters = cfg.TRAIN.epoch_iters * cfg.TRAIN.num_epoch\n",
        "    cfg.TRAIN.running_lr_encoder = cfg.TRAIN.lr_encoder\n",
        "    cfg.TRAIN.running_lr_decoder = cfg.TRAIN.lr_decoder\n",
        "\n",
        "    random.seed(cfg.TRAIN.seed)\n",
        "    torch.manual_seed(cfg.TRAIN.seed)\n",
        "\n",
        "    main(cfg, gpus)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}